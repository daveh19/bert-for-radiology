{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-tuning on Report Texts\n",
    "\n",
    "For fine-tuning, the 'simpletransformers' library is used, as it only requires a few lines of code for the training. The library can be downloaded from Github via: https://github.com/ThilinaRajapakse/simpletransformers. \n",
    "\n",
    "## Creating the enviroment\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create --name=finetuning \n",
    "conda install tensorflow-gpu pytorch scikit-learn\n",
    "\n",
    "cd transformers \n",
    "pip install .\n",
    "\n",
    "git clone https://github.com/ThilinaRajapakse/simpletransformers\n",
    "cd simpletransformers\n",
    "pip install .\n",
    "\n",
    "git clone https://github.com/NVIDIA/apex\n",
    "cd apex\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "\n",
    "conda install ipykernel pandas\n",
    "ipython kernel install --user --name=finetuning\n",
    "\n",
    "pip install python-box ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```\n",
    "'nvidia-apex' raises an error about incompatible CUDA versions. The function to check for errors was commented out. \n",
    "\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "Before fine-tuning, the data needs to be pre-processed. The 'simpletransformers' library requires the following data-structure:\n",
    "\n",
    "| text | labels |\n",
    "|------|--------|\n",
    "| 'some text for finetuning' | \\[1,0,0,1,1,0,1] |\n",
    "| 'some more texts for finetuning' | \\[0,0,0,0,0,1,1] |\n",
    "| 'even more texts for finetuning' | \\[0,1,0,1,0,0,1] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "from statistics import mean, median, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to folder containing data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATADIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data-set and bring it into the desired format (as specified above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=['Filename', 'Annotator', 'Confidence']\n",
    "data = pd.read_csv(DATADIR + 'train.csv', header=0).drop(to_drop,axis=1)\n",
    "labels=['Stauung','Verschattung','Erguss','Pneumothorax','Thoraxdrainage','ZVK','Magensonde','Tubus','Materialfehllage']\n",
    "data['labels']=data[labels].values.tolist()\n",
    "data=data.drop(labels, axis = 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATADIR + 'test.csv', header=0).drop(to_drop,axis=1)\n",
    "test['labels']=test[labels].values.tolist()\n",
    "test=test.drop(labels, axis = 1)\n",
    "test.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for performance measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultiLabelClassificationModel\n",
    "args={'output_dir': 'outputs/',\n",
    "      'cache_dir': 'cache_dir/',\n",
    "      'fp16': False,\n",
    "      'fp16_opt_level': 'O1',\n",
    "      'max_seq_length': 512,           \n",
    "      'train_batch_size': 8,\n",
    "      'gradient_accumulation_steps': 1,\n",
    "      'eval_batch_size': 12,\n",
    "      'num_train_epochs': 4,          \n",
    "      'weight_decay': 0,\n",
    "      'learning_rate': 4e-5,\n",
    "      'adam_epsilon': 1e-8,\n",
    "      'warmup_ratio': 0.06,\n",
    "      'warmup_steps': 0,\n",
    "      'max_grad_norm': 1.0,\n",
    "      'logging_steps': 50,\n",
    "      'save_steps': 2000,  \n",
    "      'evaluate_during_training': False,\n",
    "      'overwrite_output_dir': True,\n",
    "      'reprocess_input_data': True,\n",
    "      'n_gpu': 2,\n",
    "      'use_multiprocessing': True,\n",
    "      'silent': False,\n",
    "      'threshold': 0.5,\n",
    "      \n",
    "      # for long texts     \n",
    "      'sliding_window': True,\n",
    "      'tie_value': 1}\n",
    "\n",
    "model_names= ['../models/pt-radiobert-base-german-cased/', 'bert-base-german-cased', '../models/pt-radiobert-from-scratch/', 'bert-base-multilingual-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.csv\", 'w+') as f:\n",
    "    f.write('train_size,model,' + ','.join(map(str, range(1,501))) + ',\\n')\n",
    "\n",
    "for i in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000, 4500]:\n",
    "    train = data.sample(i)\n",
    "    for model_name in model_names:\n",
    "\n",
    "        model = MultiLabelClassificationModel('bert', model_name, num_labels=9, args=args)\n",
    "        model.train_model(train)\n",
    "        result, model_outputs, wrong_predictions = model.eval_model(test)\n",
    "        pred, raw =  model.predict(test.text)   \n",
    "        \n",
    "        for rep in ['models', '..', \"/\"]:\n",
    "            model_name=model_name.replace(rep, '')\n",
    "              \n",
    "        with open('results.csv', 'a') as f:\n",
    "            f.write(str(train.shape[0]) + ',' + model_name +  ',' + ','.join(map(str, raw)).replace('\\n', '') +'\\n')     \n",
    "            \n",
    "    !git add *\n",
    "    !git commit -m \"update accuracy\"\n",
    "    !git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the final models, trained on the whole train data-set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dirs = ['outputs/final/radbert/', 'outputs/final/gerbert/', 'outputs/final/fsbert/', 'outputs/final/multibert/']\n",
    "\n",
    "for i in range(3,4):\n",
    "    args[\"output_dir\"] = out_dirs[i]\n",
    "    model = MultiLabelClassificationModel('bert', model_names[i], args=args, num_labels=9)\n",
    "    model.train_model(data)\n",
    "    pred, raw =  model.predict(test.text)   \n",
    "        \n",
    "    with open('results.csv', 'a') as f:\n",
    "        f.write(str(data.shape[0]) + ',' + model_names[i] +  ',' + ','.join(map(str, raw)).replace('\\n', '') +'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on long texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On short report texts, our model does only outperform the standard german BERT model or the multilingual BERT model if the training-set for fine-tuning is very small. On larger train-sizes the value of pretraining is low. \n",
    "However, as the vocabulary of the models differs significantly, we believe, that our model will perform better on longer report texts, due to a more efficient tokenization e.g. in the context of text reports for computed tomography. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATADIR + 'ct.csv', header=0).drop(to_drop,axis=1)\n",
    "test['labels']=test[labels].values.tolist()\n",
    "test=test.drop(labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "with open('results-long-text.csv', 'w+') as f:\n",
    "    f.write('train_size,model,' + ','.join(map(str, range(1,165))) + ',\\n')\n",
    "\n",
    "model_dirs = ['outputs/final/radbert/', 'outputs/final/fsbert/', 'outputs/final/gerbert/', 'outputs/final/multibert/']\n",
    "\n",
    "for model_dir in model_dirs:\n",
    "    model = ClassificationModel('bert', model_dir, args=args)\n",
    "    pred, raw =  model.predict(test.text)   \n",
    "        \n",
    "    for rep in ['outputs', 'final', '/']:\n",
    "        model_dir=model_dir.replace(rep, '')\n",
    "              \n",
    "    with open('results-long-text.csv', 'a') as f:\n",
    "        f.write(str(4000) + ',' + model_dir +  ',' + ','.join(map(str, raw)).replace('\\n', '') +'\\n')     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast-bert",
   "language": "python",
   "name": "fast-bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Report Texts with Spacy\n",
    "link to [spaCy](https://spacy.io/models/de#de_trf_bertbasecased_lg)\n",
    "\n",
    "In order to pre-train BERT, the text needs to be pre-processed. As a starting point, there were many individual text files within the report texts. Using 'R', the texts were cleaned (white-space stripping), whereby unusable reports were excluded and put into one csv-file. The structure of the csv-file is as follows: \n",
    "\n",
    "| FILE_ADDRESS_TEXT_REPORT | TEXT |\n",
    "|--------------------------|------|\n",
    "| path/to/text/report/1234_51324_8571_341_.txt | Klinik, Fragestellung, Indikation: Beispieltext für Thorax erbeten. Befund und Beurteilung: Keine Voraufnahmen. Keine Stauung, kein Erguss, kein Pneu, keine Infiltrate. Knöcherner Thorax unauffällig |\n",
    "| path/to/text/report/61246_523424_85245_62345_.txt | Anamnese: Weiterer Beispieltext erbeten: Befund: Keine Voraufnahmen. Hier ist ein weiterer Beispieltext. Unauffälliger Befund |\n",
    "| ...  | ... |\n",
    "\n",
    "BERT requires a specific text format for pre-training, which can be created with scripts from [Google](https://github.com/google-research/bert/blob/master/create_pretraining_data.py). \n",
    "However, for the scripts to work, even the raw-input data needs a specific format. In the Google-research Git Repository it reads:  \n",
    "\n",
    "> \"The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the \"next sentence prediction\" task). Documents are delimited by empty lines.\"\n",
    "\n",
    "In our case, the text data is not in the desired format, because of which it needs to be pre-processed. This can be done as specified below. \n",
    "\n",
    "\n",
    "## Notebook summary\n",
    "As different computers and operating systems were used, the code to the set-up of the environment was always provided as a first step.  \n",
    "\n",
    "Two functions were defined: The `sentencizer` function, which splits each report text into sentences and the `fix_wrong_splits` function, which fixes wrong splits with the `sentencizer`. \n",
    "\n",
    "An example of wrong splitting is provided below: \n",
    "\n",
    "Original text:\n",
    "\n",
    "```python\n",
    "['Thorax Bedside vom 12.01.2016 Klinik, Fragestellung, Indikation: Z.n. Drainagenanlage. Frage nach Drainagenlage. Pneu? Befund und Beurteilung: Keine Voraufnahmen. 1. Kein Pneumothorax. 2. Drainage Regelrecht. 3. Zunehmende Infiltrate links. Darüber hinaus keine Befundänderung'] \n",
    "```    \n",
    "    \n",
    "This will be splitted into: \n",
    "\n",
    "```python\n",
    "['Thorax Bedside vom 12.01.2016 Klinik, Fragestellung, Indikation: Z.n.',\n",
    " 'Drainagenanlage.',\n",
    " 'Frage nach Drainagenlage.',\n",
    " 'Pneu?',\n",
    " 'Befund und Beurteilung: Keine Voraufnahmen.',\n",
    " '1.',\n",
    " 'Kein Pneumothorax.',\n",
    " '2.',\n",
    " 'Drainage Regelrecht.',\n",
    " '3.',\n",
    " 'Zunehmende Infiltrate links.',\n",
    " 'Darüber hinaus keine Befundänderung'] \n",
    "```    \n",
    "As can be seen, this is not optimal. After using `fix_wrong_splits`, it will instead be converted into: \n",
    "\n",
    "```python\n",
    "['Thorax Bedside vom 12.01.2016 Klinik, Fragestellung, Indikation: Z.n. Drainagenanlage.',\n",
    " 'Frage nach Drainagenlage.',\n",
    " 'Pneu? Befund und Beurteilung: Keine Voraufnahmen.',\n",
    " '1. Kein Pneumothorax.',\n",
    " '2. Drainage Regelrecht.',\n",
    " '3. Zunehmende Infiltrate links.',\n",
    " 'Darüber hinaus keine Befundänderung']\n",
    "```\n",
    "\n",
    "Even though this still leaves some splits unfixed, if they appear too close after each other, it greatly improves the overall performance.  \n",
    "Evaluation of the notebook took approximately 10 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the enviroment\n",
    "\n",
    "```bash\n",
    "conda create --name=text-preprocessing spacy\n",
    "conda activate text-preprocessing\n",
    "conda install ipykernel pandas\n",
    "ipython kernel install --user --name=spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "`spacy` - workhorse for sentencizing  \n",
    "`pandas` - for importing the csv file  \n",
    "`time` - for monitoring time of sentencizing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.de import German\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = German()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.read_csv('../data/cleaned-text-dump.csv', low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencizer(raw_text, nlp):\n",
    "    doc = nlp(raw_text)\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing wrong splits\n",
    "Sentences with specific endings were glued together and hardcoded into an if-statement. Then 'elif' was used to check if a sentence was very short (e.g. _'1.'_ ) and in that case to also glue it to the next sentence.  \n",
    "As the length of the document varys depending on the number of fixes, a while-loop was used instead of a for-loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_wrong_splits(sentences): \n",
    "    i=0\n",
    "    \n",
    "    while i < (len(sentences)-2): \n",
    "        if sentences[i].endswith(('Z.n.','V.a.','v.a.', 'Vd.a.' 'i.v', ' re.', \n",
    "                                  ' li.', 'und 4.', 'bds.', 'Bds.', 'Pat.', \n",
    "                                  'i.p.', 'i.P.', 'b.w.', 'i.e.L.', ' pect.', \n",
    "                                  'Ggfs.', 'ggf.', 'Ggf.',  'z.B.', 'a.e.'\n",
    "                                  'I.', 'II.', 'III.', 'IV.', 'V.', 'VI.', 'VII.', \n",
    "                                  'VIII.', 'IX.', 'X.', 'XI.', 'XII.')):\n",
    "            sentences[i:i+2] = [' '.join(sentences[i:i+2])]\n",
    "\n",
    "        elif len(sentences[i]) < 10: \n",
    "            sentences[i:i+2] = [' '.join(sentences[i:i+2])]\n",
    "\n",
    "        i+=1\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggingstep = []\n",
    "for i in range(1000): \n",
    "    loggingstep.append(i*10000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the standard sentencizer from spaCy, as it perfomes similar to other natural language processing modules, such as `de_trf_bertbasecased_lg`. If more complex text-processing is required, e.g. tokenization, the `de_trf_bertbasecased_lg` natural language processing module could be used, which can be installed via: \n",
    "\n",
    "```bash\n",
    "conda activate text-preprocessing\n",
    "pip install spacy-transformers\n",
    "python -m spacy download de_trf_bertbasecased_lg\n",
    "```\n",
    "However, only using `de_trf_bertbasecased_lg` for sentencizing is extremely slow (aprox. 10-100 times slower), because of which ist was  not used in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.clock()\n",
    "for i in range(len(texts)):\n",
    "    text = texts.TEXT[i]\n",
    "    sentences = sentencizer(text, nlp)\n",
    "    sentences = fix_wrong_splits(sentences)\n",
    "    with open('../data/report-dump.txt', 'a+') as file:\n",
    "        for sent in sentences:\n",
    "            file.write(sent + '\\n')\n",
    "        file.write('\\n')   \n",
    "    if i in loggingstep:\n",
    "        toc = time.clock()\n",
    "        print('dumped the ' + str(i) + \"th report. \" + str(toc - tic) + \"seconds passed.\")\n",
    "toc = time.clock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above-referenced steps may be executed by running the run-sentencizing.py file:\n",
    "\n",
    "```bash\n",
    "python run-sentencizing.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics\n",
    "Goal is to get extract the number of words as a word-frequency list. To split each string by words, `string.split()` can be used, but it only split by spaces and ignores special characters like colons, periods, brackets etc..\n",
    "A tokenizer can be used as a more robust method but this is very slow and therefore probably not worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count all words\n",
    "n = 0 \n",
    "file = open(r'../data/report-dump.txt', 'r',  encoding=\"utf-8-sig\")\n",
    "for word in file.read().split():\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54068691"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## count lines\n",
    "lines = 0 \n",
    "file = open(r'../data/report-dump.txt', 'r',  encoding=\"utf-8-sig\")\n",
    "for line in file:\n",
    "    lines += 1\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count individual words of file\n",
    "file = open(r'../data/report-dump.txt', 'r',  encoding=\"utf-8-sig\")\n",
    "from collections import Counter\n",
    "wordcount = Counter(file.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "counts['__Overall count__'] = []\n",
    "counts['__Overall count__'].append(['overall', n])\n",
    "counts['__individual count__'] = []\n",
    "for item in wordcount.items():\n",
    "    counts['__individual count__'].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../statistics/word-count-report-dump.json', 'w') as outfile:\n",
    "    json.dump(counts, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
